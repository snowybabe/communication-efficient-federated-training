\documentclass[journal]{IEEEtran}
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}  
\else
\fi

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage[colorlinks=true]{hyperref}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{amsmath}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{bbm}
\usepackage[noend]{algpseudocode}
\usepackage{array}
\usepackage{balance}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{threeparttable}
%\usepackage{cite}
\usepackage{subfigure}
\allowdisplaybreaks
\newcommand\algotext[1]{\end{algorithmic}#1\begin{algorithmic}[1]}


\input{yaweinewcomm}
\newtheorem{Definition}{\bf{Definition}}
\newtheorem{Property}{\bf{Property}}
\newtheorem{Theorem}{\bf{Theorem}}
\newtheorem{Corollary}{\bf{Corollary}}
\newtheorem{Lemma}{\bf{Lemma}}
\newtheorem{Remark}{\bf{Remark}}
\newtheorem{Assumption}{\bf{Assumption}}
\begin{document}


\title{Communication Efficient Training of Federated Model Over Unbalanced Labels}


\author{Yawei Zhao, Qinghe Liu, Mingming Jiang, Kunlun He
\thanks{Yawei Zhao, Qinghe Liu, Mingming Jiang, and Kunlun He are with the Medical Big Data Research Center, Chinese PLA General Hospital, Beijing, 100039, China. E-mail: \texttt{csyawei.zhao@gmail.com},  \texttt{Liuqinghe9638@163.com}, \texttt{jiangmingming1994@163.com}, \texttt{kunlunhe@plagh.org}.
}
}



% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
xxx

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
xxxx
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle


\section{Introduction}
\label{sect_introduction}
xxx




\section{Formulation}

\subsection{Personalized Representation based on Similarity Network}

Personalized models are built based on the similarity network. The similarity network measures the similarity of data distribution under data/feature/model space.  
\begin{itemize}
\item \textbf{Data space.} In the case, local datasets of every node are used to construct a \textit{kernel} matrix. The similarity of data distribution is measured by xxxxx.
\item \textbf{Feature space.} In the case, local datasets of every node are used to construct a \textit{covariance} matrix. It represents the dependence structure among features. The similarity of data distribution is measured based on the distance between covariance matrices.
\item \textbf{Model space.} In the case, local model of every node is trained by using the local dataset. The similarity of data distribution is measured based on the distance between local models.
\end{itemize}

Based on similarity under those space, the similarity network $\Gcal = \{\Ncal, \Ecal\}$ is built by using the KNN method \cite{dd}. $\Ncal=\{1,2, ..., N\}$ represents the node set, consisting of $N$ nodes. $\Ecal=\{e_{i,j} : i\in\Ncal, j {~} \text{is the node $i$'s neighbour}\}$ represents the edge set, consisting of $M$ edges. 

\begin{figure}[!t]
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\centering 
\includegraphics[width=0.97\columnwidth]{figs/figs_networkG}
\caption{Personalized representation based on similarity network}
\label{figure_xxx}
\end{figure}



\subsection{Final Formulation}
Given the mapping matrix $\M\in\RR^{d\times d_1}$ and $\N\in\RR^{d\times d_2}$, the objective problem is formulated by
\begin{align}
\min_{\substack{\{\x^{(n)}\}_{n=1}^N}} \frac{1}{N}\sum_{n\in\Ncal} f_n\lrincir{\x^{(n)}; \D_n} + \lambda \sum_{\substack{e_{i,j} \in \Ecal,\\ \forall i,j\in\Ncal}} \lrnorm{\z^{(i)} - \z^{(j)}}_p,
\end{align} subject to:
\begin{align}
\x^{(n)} = \M\x + \N \z^{(n)},{~~~~}\forall n\in\Ncal, \x\in\RR^{d_1}, \z^{(n)}\in\RR^{d_2}.
\end{align} Here, $p\in\{1,2,\infty\}$. $\M$ and $\N$ has special structure, where every row of them has at most one non-zero value, and the non-zero value is $1$.  

The formulation can be equally transformed as follows.
\begin{align}
\min_{\substack{\{\x^{(n)}\}_{n=1}^N}} \frac{1}{N}\sum_{n=1}^N f_n\lrincir{\x^{(n)}; \D_n} + \lambda \lrnorm{\Z\Q}_{1,p},
\end{align} subject to:
\begin{align}
\x^{(n)} = \M\x + \N \z^{(n)}.
\end{align} Here, $\Z\in\RR^{d_2 \times N}$, $\Q\in\RR^{N\times M}$. $N$ and $M$ represent the total number of nodes and edges in the network $\Gcal$, respectively. 
$\Z$ represents some a variable matrix, consisting of $N$ variables as columns, that is, $\Z = \left [\z^{(1)}, \z^{(2)}, ..., \z^{(N)} \right ]$. $\Q$ is the given auxiliary matrix, which has $M$ columns and every column has two non-zero values: $1$ and $-1$. Note that $\lrnorm{\cdot}_{1,p}$ is denoted by $\ell_{1,p}$ norm. Given a matrix $\U\in\RR^{d_2 \times M}$, it is defined by 
\begin{align}
\nonumber
\lrnorm{\U}_{1,p} := \sum_{m=1}^M \lrnorm{\U_{:, m}}_p.
\end{align}

\begin{figure}[!t]
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\centering 
\includegraphics[width=0.97\columnwidth]{figs/figs_Q_matrix}
\caption{$\Q$ matrix.}
\label{figure_xxx}
\end{figure}


\begin{figure}[!t]
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\centering 
\includegraphics[width=0.97\columnwidth]{figs/figs_MNmatrix}
\caption{for example: $\M$ and $\N$ }
\label{figure_xxx}
\end{figure}


\subsection{Optimization via Alternative method}

\begin{align}
\min_{\substack{\{\x^{(n)}\}_{n=1}^N}} \frac{1}{N}\sum_{n=1}^N f_n\lrincir{\x^{(n)}; \D_n} + \lambda \lrnorm{\Z\Q}_{1,p},
\end{align} subject to:
\begin{align}
\x^{(n)} = \M\x + \N \z^{(n)}.
\end{align}
The variable $\{\x^{(1)}, \x^{(2)}, ..., \x^{(N)}\}$ is obtained by alternatively optimizing $\x$ and $\{\z^{(1)}, \z^{(2)}, ..., \z^{(N)}\}$.

\textbf{Optimizing $\x$ by given $\{\z^{(1)}, \z^{(2)}, ..., \z^{(N)}\}$.} $\x$ is optimized by solving the following problem.
\begin{align}
\min_{\substack{\x\in\RR^{d_1}}} \frac{1}{N}\sum_{n=1}^N f_n\lrincir{\M\x + \N \z^{(n)}; \D_n}.
\end{align} By using the data-driven stochastic optimization method \cite{dd}, we need to perform the following problem to obtain $\x_{t+1}$, iteratively.
\begin{align}
\nonumber
& \x_{t+1} \\ \nonumber
= & \argmin_{\substack{\x\in\RR^{d_1}}} \frac{1}{N}\sum_{n=1}^N \lrangle{\M^{-1}\g_t^{(n)}, \x} + \frac{1}{2\eta}\lrnorm{\x - \x_t}^2,
\end{align} where $\g_t^{(n)}$ is a stochastic gradient of $f_n$ with $\M\x_t + \N \z_t^{(n)}$ by using data drawn from the local dataset $\D_n$.


\textbf{Optimizing $\{\z^{(1)}, \z^{(2)}, ..., \z^{(N)}\}$ by given $\x$.}




\begin{align}
\min_{\substack{\{\z^{(n)}\}_{n=1}^N}} \frac{1}{N}\sum_{n=1}^N f_n\lrincir{\M\x + \N \z^{(n)}; \D_n} + \lambda \lrnorm{\Z\Q}_{1,p}.
\end{align}  By using the data-driven stochastic optimization method \cite{dd}, we need to perform the following problem.
\begin{align}
\min_{\substack{\{\z^{(n)}\}_{n=1}^N}} \frac{1}{N}\sum_{n=1}^N \lrangle{\N^{-1} \g_t^{(n)}, \z^{(n)}} + \lambda \lrnorm{\Z\Q}_{1,p} + \frac{1}{2\eta}\lrnorm{\Z - \Z_t}_F^2.
\end{align} $\g_t^{(n)}$ is a stochastic gradient of $f_n$ with $\M\x_t + \N \z^{(n)}$ by using stochastic data  drawn from the local dataset $\D_n$. 




\textbf{On client.}  


\textbf{On server.}

\section{Communication Efficient Training}



\section{Empirical Studies}









\section*{Acknowledgment}
This work was supported by the xx. 



%\bibliographystyle{IEEEtran}  
%\bibliography{reference}


%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Yawei.pdf}}]{Yawei Zhao} is currently a Ph.D. candidate in Computer Science from the National University of Defense Technology, China. He received his B.E. degree and M.S. degree in Computer Science from the National University of Defense Technology, China, in 2013 and 2015, respectively. His research interests include asynchronous and parallel optimization algorithms, pattern recognition and machine learning.
%[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{xxx.pdf}}]{Yawei Zhao} is currently a Ph.D. candidate in Computer Science from the National University of Defense Technology, China. He received his B.E. degree and M.S. degree in Computer Science from the National University of Defense Technology, China, in 2013 and 2015, respectively. His research interests include asynchronous and parallel optimization algorithms, pattern recognition and machine learning.
%[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{jiliu.jpg}}]{Ji Liu} is currently a Ph.D. candidate in Computer Science from the National University of Defense Technology, China. He received his B.E. degree and M.S. degree in Computer Science from the National University of Defense Technology, China, in 2013 and 2015, respectively. His research interests include asynchronous and parallel optimization algorithms, pattern recognition and machine learning.
%\end{IEEEbiography}




\end{document}


